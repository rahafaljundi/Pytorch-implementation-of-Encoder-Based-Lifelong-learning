{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Finetune_SGD_EBLL.py\n"
     ]
    }
   ],
   "source": [
    " %%writefile Finetune_SGD_EBLL.py\n",
    "\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as pltD\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from ImageFolderTrainVal import *\n",
    "from AlexNet_EBLL import *\n",
    "#from test_network import *\n",
    "from SGD_Training import *\n",
    "import torch.nn.functional as F\n",
    "#from Elastic_utils import Elastic_Training\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Finetune_SGD_EBLL.py\n"
     ]
    }
   ],
   "source": [
    " %%writefile -a Finetune_SGD_EBLL.py\n",
    "\n",
    "def add_task_autoencoder_for_training(current_model):\n",
    "    \n",
    "    new_model=torch.nn.Module()\n",
    "    new_model.add_module('features',current_model.features)\n",
    "    new_model.add_module('autoecnoder',AutoEncoder(256 * 6 * 6),100)\n",
    "    new_model.add_module('classifier',current_model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Finetune_SGD_EBLL.py\n"
     ]
    }
   ],
   "source": [
    " %%writefile -a Finetune_SGD_EBLL.py\n",
    "\n",
    "\n",
    "def exp_lr_scheduler(optimizer, epoch, init_lr=0.0008, lr_decay_epoch=45):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (0.1**(epoch // lr_decay_epoch))\n",
    "    print('lr is '+str(lr))\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Finetune_SGD_EBLL.py\n"
     ]
    }
   ],
   "source": [
    " %%writefile -a Finetune_SGD_EBLL.py\n",
    "    \n",
    "    \n",
    "def distillation_loss(y, teacher_scores, T, scale):\n",
    "    \"\"\"Computes the distillation loss (cross-entropy).\n",
    "     \n",
    "    \"\"\"\n",
    "    \n",
    "    maxy,xx=y.max(1)\n",
    "    maxy=maxy.view(y.size(0),1)\n",
    "    norm_y=y-maxy.repeat(1,scale)\n",
    "    ysafe=norm_y/T\n",
    "    exsafe=torch.exp(ysafe)\n",
    "    sumex=exsafe.sum(1)\n",
    "    ######Tscores\n",
    "    maxT,xx=teacher_scores.max(1)\n",
    "    maxT=maxT.view(maxT.size(0),1)\n",
    "    teacher_scores=teacher_scores-maxT.repeat(1,scale)\n",
    "    p_teacher_scores=F.softmax(teacher_scores)   \n",
    "    p_teacher_scores=p_teacher_scores.pow(1/T)\n",
    "    p_t_sum=p_teacher_scores.sum(1)\n",
    "    p_t_sum=p_t_sum.view(p_t_sum.size(0),1)\n",
    "    p_teacher_scores=p_teacher_scores.div(p_t_sum.repeat(1,scale))\n",
    "    #  Y = sum(sum(sum(log(sumex) - sum(c .* x_safe,3),1),2),4) ;\n",
    "    \n",
    "    loss=torch.sum(torch.log(sumex)-torch.sum(p_teacher_scores*ysafe,1))\n",
    "   \n",
    "    loss=loss/teacher_scores.size(0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Finetune_SGD_EBLL.py\n"
     ]
    }
   ],
   "source": [
    " %%writefile -a Finetune_SGD_EBLL.py\n",
    "\n",
    "\n",
    "\n",
    "def train_autoencoder(model,optimizer,task_criterion,encoder_criterion, lr_scheduler,lr,dset_loaders,dset_sizes,use_gpu, num_epochs,exp_dir='./',resume='',alpha=1e-6):\n",
    "    best_model = model\n",
    "    best_acc = 10e5#arbitrary big number\n",
    "    if os.path.isfile(resume):\n",
    "        print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "        checkpoint = torch.load(resume)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        \n",
    "        print('load')\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    " \n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(resume, checkpoint['epoch']))\n",
    "    else:\n",
    "            start_epoch=0\n",
    "            print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "    \n",
    "    print(str(start_epoch))\n",
    "    #pdb.set_trace()\n",
    "\n",
    "    #------------------\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                #optimizer = lr_scheduler(optimizer, epoch,lr)\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_encoder_loss = 0.0\n",
    "            running_task_loss=0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dset_loaders[phase]:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "                    #==========\n",
    "                \n",
    "                \n",
    "\n",
    "               \n",
    "                # wrap them in Variable\n",
    "                if use_gpu:\n",
    "\n",
    "                    inputs, labels = Variable(inputs.cuda()), \\\n",
    "                        Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                \n",
    "                outputs,encoder_input,encoder_output = model(inputs)\n",
    "                encoder_input = Variable(encoder_input)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                task_loss = task_criterion(outputs, labels)\n",
    "              \n",
    "                encoder_loss=encoder_criterion(encoder_output,encoder_input)\n",
    "                # Compute distillation loss.\n",
    "                \n",
    "                total_loss=alpha*encoder_loss+task_loss\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    total_loss.backward()\n",
    "                    #print('step')\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_task_loss += task_loss.data[0]\n",
    "                running_encoder_loss+=encoder_loss.data[0]\n",
    "                running_loss+=total_loss.data[0]\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dset_sizes[phase]\n",
    "            encoder_loss=running_encoder_loss/dset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            print('{} Encoder LOSS: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, encoder_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and encoder_loss< best_acc:\n",
    "              \n",
    "                del labels\n",
    "                del inputs\n",
    "                del task_loss\n",
    "                \n",
    "                del preds\n",
    "                best_acc = encoder_loss\n",
    "                \n",
    "                #best_model = copy.deepcopy(model)\n",
    "                torch.save(model,os.path.join(exp_dir, 'best_model.pth.tar'))\n",
    "                \n",
    "        #epoch_file_name=exp_dir+'/'+'epoch-'+str(epoch)+'.pth.tar'\n",
    "        epoch_file_name=exp_dir+'/'+'epoch'+'.pth.tar'\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'encoder_loss':encoder_loss,\n",
    "            'arch': 'alexnet',\n",
    "            'model': model,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "                },epoch_file_name)\n",
    "        print()\n",
    "\n",
    "\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Finetune_SGD_EBLL.py\n"
     ]
    }
   ],
   "source": [
    " %%writefile -a Finetune_SGD_EBLL.py\n",
    "\n",
    "\n",
    "def train_model_ebll(model,original_model, criterion, code_criterion,optimizer, lr_scheduler,lr,dset_loaders,dset_sizes,use_gpu, num_epochs,exp_dir='./',resume='',temperature=2,alpha=1e-6):\n",
    "    print('dictoinary length'+str(len(dset_loaders)))\n",
    "    #set orginal model to eval mode\n",
    "    original_model.eval()\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "    if os.path.isfile(resume):\n",
    "        print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "        checkpoint = torch.load(resume)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        \n",
    "        print('load')\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    " \n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(resume, checkpoint['epoch']))\n",
    "    else:\n",
    "            start_epoch=0\n",
    "            print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "    \n",
    "    print(str(start_epoch))\n",
    "    #pdb.set_trace()\n",
    "\n",
    "    #------------------\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                optimizer = lr_scheduler(optimizer, epoch,lr)\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_code_loss=0.0\n",
    "            # Iterate over data.\n",
    "            for data in dset_loaders[phase]:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "                    #==========\n",
    "                if phase == 'train':    \n",
    "                    original_inputs=inputs.clone()\n",
    "                \n",
    "\n",
    "               \n",
    "                # wrap them in Variable\n",
    "                if use_gpu:\n",
    "                    if phase == 'train':\n",
    "                        original_inputs = original_inputs.cuda()\n",
    "                        original_inputs = Variable(original_inputs, requires_grad=False)\n",
    "                    inputs, labels = Variable(inputs.cuda()), \\\n",
    "                        Variable(labels.cuda())\n",
    "                else:\n",
    "                    if phase == 'train': \n",
    "                       \n",
    "                        original_inputs = Variable(original_inputs, requires_grad=False)\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "                original_model.zero_grad()\n",
    "                # forward\n",
    "                #tasks_outputs and target_logits are lists of outputs for each task in the previous model and current model\n",
    "                orginal_logits,orginal_codes =original_model(original_inputs)\n",
    "                 # Move to same GPU as current model.\n",
    "                target_logits = [Variable(item.data, requires_grad=False)\n",
    "                                     for item in orginal_logits]\n",
    "                \n",
    "                target_codes = [Variable(item.data, requires_grad=False)\n",
    "                     for item in orginal_codes]\n",
    "                del orginal_logits\n",
    "                scale = [item.size(-1) for item in target_logits]\n",
    "                tasks_outputs,tassk_codes = model(inputs)\n",
    "                _, preds = torch.max(tasks_outputs[-1].data, 1)\n",
    "                task_loss = criterion(tasks_outputs[-1], labels)\n",
    "                \n",
    "                # Compute distillation loss.\n",
    "                dist_loss = 0.0\n",
    "                code_loss = 0.0\n",
    "                # Apply distillation loss to all old tasks.\n",
    "                \n",
    "                if phase == 'train': \n",
    "                    for idx in range(len(target_logits)):\n",
    "                        dist_loss += distillation_loss(tasks_outputs[idx], target_logits[idx], temperature, scale[idx])\n",
    "                    # compute code loss for the previous tasks\n",
    "                    for idx in range(len(target_codes)):\n",
    "                        code_loss += code_criterion(tassk_codes[idx], target_codes[idx])\n",
    "     \n",
    "                total_loss=dist_loss+task_loss+alpha*code_loss\n",
    "                #backprobagate and update\n",
    "                if phase == 'train':\n",
    "                    total_loss.backward()\n",
    "                    \n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += task_loss.data[0]\n",
    "                if phase == 'train':\n",
    "                    running_code_loss += code_loss.data[0]\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dset_sizes[phase]\n",
    "            epoch_code_loss=running_code_loss/dset_sizes[phase]\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            if phase == 'train':\n",
    "               \n",
    "                print('FIRST TASK CODE LOSS', str(epoch_code_loss))\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                del tasks_outputs\n",
    "                del labels\n",
    "                del inputs\n",
    "                del task_loss\n",
    "                del preds\n",
    "                best_acc = epoch_acc\n",
    "                #best_model = copy.deepcopy(model)\n",
    "                torch.save(model,os.path.join(exp_dir, 'best_model.pth.tar'))\n",
    "                \n",
    "        #epoch_file_name=exp_dir+'/'+'epoch-'+str(epoch)+'.pth.tar'\n",
    "        epoch_file_name=exp_dir+'/'+'epoch'+'.pth.tar'\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'epoch_acc':epoch_acc,\n",
    "            'arch': 'alexnet',\n",
    "            'model': model,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "                },epoch_file_name)\n",
    "        print()\n",
    "\n",
    "   \n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Finetune_SGD_EBLL.py\n"
     ]
    }
   ],
   "source": [
    " %%writefile -a Finetune_SGD_EBLL.py\n",
    "\n",
    "def fine_tune_Adam_Autoencoder(dataset_path,previous_task_model_path,init_model_path='',exp_dir='',batch_size=200, num_epochs=100,lr=0.1,init_freeze=1,lr_decay_epoch=45,pretrained=True,alpha=1e-6):\n",
    "    \"\"\"call encoder training using Adadelta optimizer\"\"\"\n",
    "    print('lr is ' + str(lr))\n",
    "    \n",
    "    dsets = torch.load(dataset_path)\n",
    "    dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=batch_size,\n",
    "                                               shuffle=True, num_workers=4)\n",
    "                for x in ['train', 'val']}\n",
    "    dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "    dset_classes = dsets['train'].classes\n",
    "\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    resume=os.path.join(exp_dir,'epoch.pth.tar')\n",
    "    \n",
    "    if os.path.isfile(resume):\n",
    "            checkpoint = torch.load(resume)\n",
    "            model_ft = checkpoint['model']\n",
    "    \n",
    "    else: \n",
    "        if not os.path.isfile(previous_task_model_path):\n",
    "            model_ft = models.alexnet(pretrained=pretrained)\n",
    "\n",
    "        else:\n",
    "            model_ft=torch.load(previous_task_model_path)\n",
    "        if hasattr(model_ft,'reg_params'):\n",
    "            model_ft.reg_params=None\n",
    "      \n",
    "        model_ft=AlexNet_ENCODER(model_ft)\n",
    "\n",
    "       \n",
    "        num_ftrs = model_ft.classifier[6].in_features \n",
    "\n",
    "    if use_gpu:\n",
    "        model_ft = model_ft.cuda()\n",
    "       \n",
    "    #train the autoencoder using two losses: task loss on the reconstructed data and reconstruction MSE\n",
    "    task_criterion = nn.CrossEntropyLoss()\n",
    "    encoder_criterion = nn.MSELoss()\n",
    "\n",
    "   \n",
    "    optimizer_ft =  optim.Adadelta(model_ft.autoencoder.parameters(),  lr)\n",
    "\n",
    "        \n",
    "    \n",
    "   \n",
    "    model_ft = train_autoencoder(model_ft,optimizer_ft, task_criterion,encoder_criterion,exp_lr_scheduler,lr, dset_loaders,dset_sizes,use_gpu,num_epochs,exp_dir,resume,alpha=alpha)\n",
    "\n",
    "\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Finetune_SGD_EBLL.py\n"
     ]
    }
   ],
   "source": [
    " %%writefile -a Finetune_SGD_EBLL.py\n",
    "\n",
    "\n",
    "def fine_tune_SGD_EBLL(dataset_path,previous_task_model_path,autoencoder_model_path,init_model_path='',exp_dir='',batch_size=200, num_epochs=100,lr=0.0004,init_freeze=1,lr_decay_epoch=45,pretrained=True,alpha=1e-6):\n",
    "    \"\"\"call EBLL training on a given task in a sequence\"\"\"\n",
    "    print('lr is ' + str(lr))\n",
    "    #data loader\n",
    "    dsets = torch.load(dataset_path)\n",
    "    dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=batch_size,\n",
    "                                               shuffle=True, num_workers=4)\n",
    "                for x in ['train', 'val']}\n",
    "    dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "    dset_classes = dsets['train'].classes\n",
    "\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    resume=os.path.join(exp_dir,'epoch.pth.tar')\n",
    "    #load the recently trained autoencoder\n",
    "    autoencoder_model=torch.load(autoencoder_model_path)\n",
    "    if os.path.isfile(resume):\n",
    "            checkpoint = torch.load(resume)\n",
    "            model_ft = checkpoint['model']\n",
    "            previous_model=torch.load(previous_task_model_path)\n",
    "            if not (type(previous_model) is AlexNet_EBLL):\n",
    "                previous_model=AlexNet_EBLL(previous_model,autoencoder_model.autoencoder)\n",
    "            original_model=copy.deepcopy(previous_model)\n",
    "            del checkpoint\n",
    "            del previous_model\n",
    "    else: \n",
    "        if not os.path.isfile(previous_task_model_path):\n",
    "            model_ft = models.alexnet(pretrained=pretrained)\n",
    "\n",
    "        else:\n",
    "            model_ft=torch.load(previous_task_model_path)\n",
    "        if hasattr(model_ft,'reg_params'):\n",
    "            model_ft.reg_params=None\n",
    "        if not (type(model_ft) is AlexNet_EBLL):\n",
    "            #make a new Alexnet_ebll instant that has additionally the trained autoencoder to preserve the codes\n",
    "            model_ft=AlexNet_EBLL(model_ft,autoencoder_model.autoencoder)\n",
    "        else:\n",
    "            #add the new autoencoder to previous Alexnet_ebll\n",
    "            #in case of a sequence longer than 2\n",
    "            \n",
    "            model_ft.autoencoders.add_module(str(len(model_ft.autoencoders._modules.items())), autoencoder_model.autoencoder.encode)\n",
    "\n",
    "        original_model=copy.deepcopy(model_ft)\n",
    "        num_ftrs = model_ft.classifier[6].in_features \n",
    "        #initialize a new head from the trained feature extractor, or randomly\n",
    "        if not init_freeze:   \n",
    "\n",
    "            model_ft.classifier.add_module(str(len(model_ft.classifier._modules)),nn.Linear(num_ftrs, len(dset_classes)))\n",
    "        else:\n",
    "\n",
    "            init_model=torch.load(init_model_path)\n",
    "            model_ft.classifier.add_module(str(len(model_ft.classifier._modules)), init_model.classifier[6])\n",
    "            del init_model\n",
    " \n",
    "        if not os.path.exists(exp_dir):\n",
    "            os.makedirs(exp_dir)\n",
    "    if use_gpu:\n",
    "        model_ft = model_ft.cuda()\n",
    "        original_model=original_model.cuda()\n",
    "   \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    encoder_criterion = nn.MSELoss()\n",
    "    # Observe that all parameters are being optimized\n",
    "    #optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.0008, momentum=0.9)\n",
    "    params = list(model_ft.features.parameters()) + list(model_ft.classifier.parameters())\n",
    "    optimizer_ft =  optim.SGD(params, lr, momentum=0.9)\n",
    "\n",
    "    model_ft = train_model_ebll(model_ft,original_model, criterion, encoder_criterion,optimizer_ft,exp_lr_scheduler,lr, dset_loaders,dset_sizes,use_gpu,num_epochs,exp_dir,resume,alpha=alpha)\n",
    "    \n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Finetune_SGD_EBLL.py\n"
     ]
    }
   ],
   "source": [
    " %%writefile -a Finetune_SGD_EBLL.py\n",
    "    \n",
    "    \n",
    "def test_model(model_path,dataset_path,batch_size=200,task_index=0,check=0):\n",
    "    model=torch.load(model_path)\n",
    "    if check:\n",
    "        model=model['model']\n",
    "    model=model.cuda()\n",
    "    dsets = torch.load(dataset_path)\n",
    "    dset_loaders = {x: torch.utils.data.DataLoader(dsets[x],batch_size ,\n",
    "                                                   shuffle=True, num_workers=4)\n",
    "                for x in ['train', 'val']}\n",
    "    dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "    dset_classes = dsets['train'].classes\n",
    "    class_correct = list(0. for i in range(len(dset_classes)))\n",
    "    class_total = list(0. for i in range(len(dset_classes)))\n",
    "    for data in dset_loaders['val']:\n",
    "        images, labels = data\n",
    "        images=images.cuda()\n",
    "        labels=labels.cuda()\n",
    "        outputs,codes = model(Variable(images))\n",
    "        _, predicted = torch.max(outputs[task_index].data, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        #pdb.set_trace()\n",
    "        for i in range(len(predicted)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i]\n",
    "            class_total[label] += 1\n",
    "        del images\n",
    "        del labels\n",
    "        del outputs\n",
    "        del data\n",
    "    if 0:\n",
    "        for i in range(len(dset_classes)):\n",
    "            print('Accuracy of %5s : %2d %%' % (\n",
    "            dset_classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "    accuracy=np.sum(class_correct)*100/np.sum(class_total)\n",
    "    print('Accuracy: ' +str(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Finetune_SGD_EBLL.py\n"
     ]
    }
   ],
   "source": [
    " %%writefile -a Finetune_SGD_EBLL.py\n",
    "\n",
    "\n",
    "def fine_tune_freeze(dataset_path,model_path,exp_dir,batch_size=100, num_epochs=100,lr=0.0004):\n",
    "    \"\"\"train the new head alone, in case of warmup phase\"\"\"\n",
    "    print('lr is ' + str(lr))\n",
    "    \n",
    "    dsets = torch.load(dataset_path)\n",
    "    dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=batch_size,\n",
    "                                               shuffle=True, num_workers=4)\n",
    "                for x in ['train', 'val']}\n",
    "    dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "    dset_classes = dsets['train'].classes\n",
    "\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    resume=os.path.join(exp_dir,'epoch.pth.tar')\n",
    "    if os.path.isfile(resume):\n",
    "            checkpoint = torch.load(resume)\n",
    "            model_ft = checkpoint['model']\n",
    "    if not os.path.isfile(model_path):\n",
    "        model_ft = models.alexnet(pretrained=True)\n",
    "       \n",
    "    else:\n",
    "        model_ft=torch.load(model_path)\n",
    "    if type(model_ft) is AlexNet_EBLL:\n",
    "        \n",
    "        this_model_ft=models.alexnet(pretrained=True)\n",
    "        \n",
    "        this_model_ft.features=model_ft.features\n",
    "        this_model_ft.classifier=model_ft.classifier\n",
    "        model_ft=this_model_ft\n",
    "        num_ftrs = model_ft.classifier[6].in_features \n",
    "        keep_poping=True\n",
    "        while keep_poping:\n",
    "            x=model_ft.classifier._modules.popitem()\n",
    "            if x[0]=='6':\n",
    "                keep_poping=False\n",
    "    else:            \n",
    "        num_ftrs = model_ft.classifier[6].in_features \n",
    "        \n",
    "    model_ft.classifier._modules['6'] = nn.Linear(num_ftrs, len(dset_classes))    \n",
    "    if not os.path.exists(exp_dir):\n",
    "        os.makedirs(exp_dir)\n",
    "    if use_gpu:\n",
    "        model_ft = model_ft.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    \n",
    "    optimizer_ft =  optim.SGD(model_ft.classifier._modules['6'].parameters(), lr, momentum=0.9)\n",
    "\n",
    "        \n",
    "    \n",
    "  \n",
    "    model_ft = train_model(model_ft, criterion, optimizer_ft,exp_lr_scheduler,lr, dset_loaders,dset_sizes,use_gpu,num_epochs,exp_dir,resume)\n",
    "    \n",
    "    return model_ft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
